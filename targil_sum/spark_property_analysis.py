# -*- coding: utf-8 -*-
"""spark-property-analysis.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IigE6GugijGX9n9GlIlv2iOvlMMZx4UM

# Spark Property Data Analysis

This notebook performs analysis on fake property data using PySpark in Google Colab.

written and submitted by David Koplev 208870279 and Rotem Kashani 209073352

## Cell 1: Install necessary libraries
This cell installs the required PySpark library
"""

!pip install pyspark

"""## Cell 2: Import libraries and set up Spark session
This cell imports necessary libraries and creates a Spark session
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, year, desc, collect_list, struct
import sqlite3
from google.colab import files
import io

# Create a Spark session
spark = SparkSession.builder \
    .appName("Property Data Analysis") \
    .getOrCreate()

"""## Cell 3: Upload and read JSON file
This cell handles file upload and reads the JSON data into a Spark DataFrame
"""

import os

filename = 'fake_property_data.json'
if not os.path.exists(filename):
    # Upload the JSON file if it does not exist
    uploaded = files.upload()
    filename = next(iter(uploaded))

# Upload the SQLite database file if it does not exist
db_filename = 'price_paid_records.db'
if not os.path.exists(db_filename):
    uploaded_db = files.upload()
    db_filename = next(iter(uploaded_db))



# Read the JSON file
df = spark.read.option("multiline", "true").json(filename)

"""## Cell 4: Data preprocessing
This cell converts data types and prepares the DataFrame for analysis
"""

# Convert 'price' to numeric and 'transfer_date' to date
df = df.withColumn("price", col("price").cast("double")) \
       .withColumn("transfer_date", col("transfer_date").cast("date"))

"""## Cell 5: Data analysis
This cell performs the main analysis to find top counties by average property price for each year
"""

from pyspark.sql import functions as F
from pyspark.sql.window import Window
# Question: What are the top 10 counties with the highest average property prices for each year?
# Step 1: Add a year column extracted from transfer_date
df = df.withColumn("year", F.year("transfer_date"))

# Step 2: Calculate average price per county per year
avg_prices = df.groupBy("year", "county") \
               .agg(F.avg("price").alias("avg_price"))

# Step 3: Rank counties within each year based on average price
window_spec = Window.partitionBy("year").orderBy(F.desc("avg_price"))

# Step 4: Rank counties with row_number to handle ties strictly
ranked_counties = avg_prices.withColumn("rank", F.row_number().over(window_spec))

# Step 5: Select top 10 counties for each year
top_counties_by_year = ranked_counties.filter(F.col("rank") <= 10) \
                                      .groupBy("year") \
                                      .agg(F.collect_list(F.struct("county", "avg_price")).alias("top_counties"))

# Show the results
top_counties_by_year.show(truncate=False)

"""## Cell 6: Export results to SQLite
This cell saves the analysis results to an SQLite database
"""

from pyspark.sql.functions import to_json, col
from pyspark.sql.types import StringType
import pandas as pd
import sqlite3

# Assuming top_counties_by_year is your Spark DataFrame

# Convert the 'top_counties' column to a JSON string
top_counties_by_year = top_counties_by_year.withColumn('top_counties', to_json(col('top_counties')))

# Ensure all columns are cast to types SQLite can handle
top_counties_by_year = top_counties_by_year.select(
    col('year').cast(StringType()),
    col('top_counties').cast(StringType())
)

# Convert to Pandas DataFrame
pandas_df = top_counties_by_year.toPandas()

# Connect to SQLite database
conn = sqlite3.connect('price_paid_records.db')

# Save results to SQLite database using pandas, append to existing database
pandas_df.to_sql('top_counties_by_year', conn, if_exists='append', index=False)

# Close the database connection
conn.close()

print("Analysis complete. Results appended to price_paid_records.db")

"""## Cell 7: Download results and clean up
This cell downloads the SQLite database and stops the Spark session
"""

# Download the database file
files.download('price_paid_records.db')

# Stop the Spark session
spark.stop()